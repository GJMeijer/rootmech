---
title: "power_normal_scaled_fit"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{power_normal_scaled_fit}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Problem definitions

We want to fit a series of $x$,$y$ data using a power law which describes the mean of $y$ at each value of $x$:

$$
\bar{y} = y_0 x^\beta
$$

Using the definition of the mean of the normal distribution, at each value of $x$ the following must satisfied:

$$
\mu = y_0 x^\beta
$$

The standard deviation is assumed to scale with the mean, i.e:

$$
\sigma = \sigma_0 x^\beta \propto \mu
$$

The probability $p$ following the normal distribution:

$$ 
p = \frac{1}{\sigma \sqrt{2 \pi}} \exp \left[- \frac{1}{2} \left( \frac{y - \mu}{\sigma} \right)^2 \right]
$$

which, with the above expressions, can be rewritten as:

$$
p = \frac{1}{\sigma_0 x^\beta\sqrt{2 \pi}} \exp \left[ - \frac{1}{2\sigma_0^2} \left( \frac{y}{x^\beta} - y_0 \right)^2 \right]
$$

Solve for the best fitting parameters $y_0$, $\beta$ and $\sigma_0$ by maximising the likelihood:

$$
\mathcal{L} = \prod p^w
$$

where $w$ is the weighting of each observation. We can alternatively maximise the loglikelihood:

$$
\log\mathcal{L} = \sum w \log p
$$

Writing this out fully:

$$
\log\mathcal{L} = 
-\sum w \left(\log\sigma_0 + \beta \log x + \frac{1}{2} \log(2\pi)\right) -
\sum \frac{w}{2\sigma_0^2} \left[\frac{y}{x^\beta} - y_0\right]^2
$$

# Coefficients

Define the following coefficients to make writing equations easier:

$$
c_1 = \sum w
$$

$$
c_2 = \sum w \log x
$$

$$ 
c_3 = \sum w x^{-\beta} y
\qquad
\frac{\partial c_3}{\partial\beta} = -c_4
$$

$$ 
c_4 = \sum w x^{-\beta} y \log x
\qquad
\frac{\partial c_4}{\partial\beta} = -c_5
$$

$$ 
c_5 = \sum w x^{-\beta} y \log^2 x
$$

$$
c_6 = \sum w x^{-2\beta} y^2
\qquad
\frac{\partial c_6}{\partial\beta} = -2 c_7
$$

$$
c_7 = \sum w x^{-2\beta} y^2 \log x
\qquad
\frac{\partial c_7}{\partial\beta} = -2 c_8
$$

$$
c_8 = \sum w x^{-2\beta} y^2 \ log^2 x
$$

With these parameters, the loglikelihood function can be rewritten as:

$$
\log\mathcal{L} = 
-c_1 \left(\log\sigma_0 + \frac{1}{2} \log(2\pi) \right) -
\beta c_2 -
\frac{ \left( c_6 - 2 y_0 c_3 + c_1 y_0^2 \right) }{2\sigma_0^2}
$$

# Loglikelihood solving

The maximum loglikelihood can be found where the partial derivatives of $\log\mathcal{L}$ with respect to $y_0$, $\beta$ and $\sigma_0$ are zero. The derivative with respect to $y_0$:

$$
\frac{\partial\log\mathcal{L}}{\partial y_0} = 
\frac{c_3 - c_1 y_0}{\sigma_0^2}
= 0
$$

Solving this for $y_0$ gives:

$$
y_0 = \frac{c_3}{c_1}
$$

Substituting this result back into the equation for $\log\mathcal{L}$

$$
\log\mathcal{L} = 
-c_1 \left(\log\sigma_0 + \frac{1}{2} \log(2\pi) \right) -
\beta c_2 -
\frac{1}{2\sigma_0^2} \left[c_6 - \frac{c_3^2}{c_1} \right]
$$

Now taking the derivative with respect to $\sigma_0$:

$$
\frac{\partial\log\mathcal{L}}{\partial \sigma_0} = 
\frac{1}{\sigma_0^3} \left[c_6 - \frac{c_3^2}{c_1} \right] - 
\frac{c_1}{\sigma_0}
= 0
$$

Solving this for $\sigma_0^2$ gives:

$$
\sigma_0^2 = \frac{c_6}{c_1} - \frac{c_3^2}{c_1^2}
$$

Substituting this back into the expression for the loglikelihood:

$$
\log\mathcal{L} = 
-\frac{c_1}{2} \left[ \log\left(\frac{c_6}{c_1} - \frac{c_3^2}{c_1^2}\right) +  \log(2\pi) + 1 \right] -
\beta c_2 
$$

The derivative with respect to $\beta$ is now:

$$
\frac{\partial\log\mathcal{L}}{\partial \beta} = 
\frac{c_1 (c_1 c_7 - c_3 c_4)}{c_1 c_6 - c_3^2} - c_2
= 0
$$

Which can be solved for the final unknown $\beta$. When using gradient descent methods, the second-order derivative equals:

$$
\frac{\partial\log\mathcal{L}}{\partial \beta^2} = 
\frac{c_1 (-2 c_1 c_8 + c_4^2 + c_3 c_5)}{c_1 c_6 - c_3^2} + 
\frac{2 c_1 (c_1 c_7 - c_3 c_4)^2}{(c_1 c_6 - c_3^2)^2}
$$

# Loglikelihood derivatives

The partial derivatives of the loglikelihood function can be used to estimate the covariance matrix of the fitting parameters.

The first-order partial derivatives are:

$$
\frac{\partial\log\mathcal{L}}{\partial y_0} = 
\frac{1}{\sigma^2} \left( c_3 - c_1 y_0 \right)
$$

$$
\frac{\partial\log\mathcal{L}}{\partial \beta} = 
\frac{1}{\sigma^2} \left( c_7 - c_4 y_0 \right) - c_2
$$

$$
\frac{\partial\log\mathcal{L}}{\partial \sigma_0} = 
\frac{-c_1}{\sigma_0} + \frac{1}{\sigma_0^3} \left(c_6 - 2 c_3 y_0 + c_1 y_0^2 \right)
$$

The second-order partial derivatives:

$$
\frac{\partial^2\log\mathcal{L}}{\partial y_0^2} = 
\frac{-c_1}{y_0}
$$

$$
\frac{\partial^2\log\mathcal{L}}{\partial y_0 \partial \beta} = 
\frac{-c_4}{\sigma_0^2}
$$

$$
\frac{\partial^2\log\mathcal{L}}{\partial y_0 \partial \sigma_0} = 
\frac{2}{\sigma_0^3} \left( c_1 y_0 - c_3 \right)
$$

$$
\frac{\partial^2\log\mathcal{L}}{\partial \beta^2} = 
\frac{1}{\sigma_0^2} \left( c_5 y_0 - 2 c_8 \right)
$$

$$
\frac{\partial^2\log\mathcal{L}}{\partial \beta \partial \sigma_0} = 
\frac{2}{\sigma_0^3} \left( c_4 y_0 - c_7 \right)
$$

$$
\frac{\partial^2\log\mathcal{L}}{\partial \sigma_0^2} = 
\frac{c_1}{\sigma_0^2} - \frac{3}{\sigma_0^4} \left( c_6 - 2 c_3 y_0 + c_1 y_0^2 \right)
$$
